## LocalPV

1. 在持久化存储领域，用户呼声最高的定制化需求，莫过于支持“本地”持久化存储了。
也就是说，用户希望 Kubernetes 能够直接使用宿主机上的本地磁盘目录，而不依赖于远程存储服务，来提供“持久化”的容器 Volume。
这样做的好处很明显，由于这个 Volume 直接使用的是本地磁盘，尤其是 SSD 盘，它的读写性能相比于大多数远程存储来说，要好得多。
这个需求对本地物理服务器部署的私有 Kubernetes 集群来说，非常常见。

需要明确的是，Local Persistent Volume 并不适用于所有应用。事实上，它的适用范围非常固定，
比如：高优先级的系统应用，需要在多个不同节点上存储数据，并且对 I/O 较为敏感。
典型的应用包括：分布式数据存储比如 MongoDB、Cassandra 等，分布式文件系统比如 GlusterFS、Ceph 等，
以及需要在本地磁盘上进行大量数据缓存的分布式应用。

另一方面，相比于正常的 PV，一旦这些节点宕机且不能恢复时，Local Persistent Volume 的数据就可能丢失。
这就要求使用 Local Persistent Volume 的应用必须具备数据备份和恢复的能力，
允许你把这些数据定时备份在其他位置。

---

2. Local Persistent Volume 的设计，主要面临两个难点。
第一个难点在于：如何把本地磁盘抽象成 PV。

Local Persistent Volume，不就等同于 hostPath 加 NodeAffinity 吗？
比如，一个 Pod 可以声明使用类型为 Local 的 PV，而这个 PV 其实就是一个 hostPath 类型的 Volume。
如果这个 hostPath 对应的目录，已经在节点 A 上被事先创建好了。
那么，我只需要再给这个 Pod 加上一个 nodeAffinity=nodeA，不就可以使用这个 Volume 了吗？

事实上，你绝不应该把一个宿主机上的目录当作 PV 使用。这是因为，这种本地目录的存储行为完全不可控，它所在的磁盘随时都可能被应用写满，
甚至造成整个宿主机宕机。而且，不同的本地目录之间也缺乏哪怕最基础的 I/O 隔离机制。
所以，一个 Local Persistent Volume 对应的存储介质，一定是一块额外挂载在宿主机的磁盘或者块设备
（“额外”的意思是，它不应该是宿主机根目录所使用的主硬盘）。
这个原则，我们可以称为“一个 PV 一块盘”。

第二个难点在于：调度器如何保证 Pod 始终能被正确地调度到它所请求的 Local Persistent Volume 所在的节点上呢？
对于 Local PV 来说，节点上可供使用的磁盘（或者块设备），必须是运维人员提前准备好的。
它们在不同节点上的挂载情况可以完全不同，甚至有的节点可以没这种磁盘。
所以，这时候，调度器就必须能够知道所有节点与 Local Persistent Volume 对应的磁盘的关联关系，然后根据这个信息来调度 Pod。
这个原则，我们可以称为“在调度的时候考虑 Volume 分布”。
在 Kubernetes 的调度器里，有一个叫作 VolumeBindingChecker 的过滤条件专门负责这个事情。
在 Kubernetes v1.11 中，这个过滤条件已经默认开启了。

在我们部署的私有环境中，你有两种办法来完成这个步骤。
第一种，当然就是给你的宿主机挂载并格式化一个可用的本地磁盘，这也是最常规的操作；
第二种，对于实验环境，你其实可以在宿主机上挂载几个 RAM Disk（内存盘）来模拟本地磁盘。
以第二种为例：
首先，在名叫 node-1 的宿主机上创建一个挂载点，比如 /mnt/disks；然后，用几个 RAM Disk 来模拟本地磁盘，如下所示：
```shell
# 在node-1上执行
$ mkdir /mnt/disks
$ for vol in vol1 vol2 vol3; do
    mkdir /mnt/disks/$vol
    mount -t tmpfs $vol /mnt/disks/$vol
done
```

需要注意的是，如果你希望其他节点也能支持 Local Persistent Volume 的话，那就需要为它们也执行上述操作，
并且确保这些磁盘的名字（vol1、vol2 等）都不重复。

制定PV:
```yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: example-pv
spec:
  capacity:
    storage: 5Gi
  volumeMode: Filesystem
  accessModes:
  - ReadWriteOnce
  persistentVolumeReclaimPolicy: Delete
  storageClassName: local-storage
  local:
    path: /mnt/disks/vol1
  nodeAffinity:
    required:
      nodeSelectorTerms:
      - matchExpressions:
        - key: kubernetes.io/hostname
          operator: In
          values:
          - node-1
```
可以看到，这个 PV 的定义里：local 字段，指定了它是一个 Local Persistent Volume；
而 path 字段，指定的正是这个 PV 对应的本地磁盘的路径，即：/mnt/disks/vol1。

在这个 PV 的定义里，需要有一个 nodeAffinity 字段指定 node-1 这个节点的名字。
这样，调度器在调度 Pod 的时候，就能够知道一个 PV 与节点的对应关系，从而做出正确的选择。
这正是 Kubernetes 实现“在调度的时候就考虑 Volume 分布”的主要方法。

使用 PV 和 PVC 的最佳实践，是你要创建一个 StorageClass 来描述这个 PV.
```yaml
kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: local-storage
provisioner: kubernetes.io/no-provisioner
volumeBindingMode: WaitForFirstConsumer
```
这个 StorageClass 的名字，叫作 local-storage。需要注意的是，在它的 provisioner 字段，我们指定的是 no-provisioner。
这是因为 Local Persistent Volume 目前尚不支持 Dynamic Provisioning，所以它没办法在用户创建 PVC 的时候，就自动创建出对应的 PV。
也就是说，我们前面创建 PV 的操作，是不可以省略的。

与此同时，这个 StorageClass 还定义了一个 volumeBindingMode=WaitForFirstConsumer 的属性。
它是 Local Persistent Volume 里一个非常重要的特性，即：延迟绑定。
通过这个延迟绑定机制，原本实时发生的 PVC 和 PV 的绑定过程，就被延迟到了 Pod 第一次调度的时候在调度器中进行，
从而保证了这个绑定结果不会影响 Pod 的正常调度。

创建一个pvc与pv进行绑定，这样就可以让 Pod 使用到上面定义好的 Local Persistent Volume 了。
```yaml
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: example-local-claim
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 5Gi
  storageClassName: local-storage
```
创建一个POD进行绑定
```yaml
kind: Pod
apiVersion: v1
metadata:
  name: example-pv-pod
spec:
  volumes:
    - name: example-pv-storage
      persistentVolumeClaim:
       claimName: example-local-claim
  containers:
    - name: example-pv-container
      image: nginx
      ports:
        - containerPort: 80
          name: "http-server"
      volumeMounts:
        - mountPath: "/usr/share/nginx/html"
          name: example-pv-storage
```
 而我们一旦使用 kubectl create 创建这个 Pod，就会发现，我们前面定义的 PVC，会立刻变成 Bound 状态，与前面定义的 PV 绑定在了一起。
 也就是说，在我们创建的 Pod 进入调度器之后，“绑定”操作才开始进行。

 像 Kubernetes 这样构建出来的、基于本地存储的 Volume，完全可以提供容器持久化存储的功能。
 所以，像 StatefulSet 这样的有状态编排工具，也完全可以通过声明 Local 类型的 PV 和 PVC，来管理应用的存储状态。

---

 3. 需要注意的是，我们上面手动创建 PV 的方式，即 Static 的 PV 管理方式，在删除 PV 时需要按如下流程执行操作：
 - 删除使用这个 PV 的 Pod；
 - 从宿主机移除本地磁盘（比如，umount 它）；
 - 删除 PVC；
 - 删除 PV。
 
 如果不按照这个流程的话，这个 PV 的删除就会失败。
 当然，由于上面这些创建 PV 和删除 PV 的操作比较繁琐，Kubernetes 其实提供了一个 Static Provisioner 来帮助你管理这些 PV。

 比如，我们现在的所有磁盘，都挂载在宿主机的 /mnt/disks 目录下。那么，当 Static Provisioner 启动后，它就会通过 DaemonSet，
 自动检查每个宿主机的 /mnt/disks 目录。然后，调用 Kubernetes API，为这些目录下面的每一个挂载，创建一个对应的 PV 对象出来。
 provisioner 也会负责前面提到的 PV 的删除工作。

 而这个 provisioner 本身，其实也是一个我们前面提到过的External Provisioner，它的部署方法，在对应的文档里有详细描述。
 
 这部分内容，就留给你自行探索了：
 https://github.com/kubernetes-retired/external-storage/tree/master/local-volume