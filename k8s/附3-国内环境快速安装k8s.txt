一.安装docker
先执行apt-
1. sudo apt install docker.io
2. docker version查看版本

二. 安装kubelet kubeadm kubectl
ubuntu
1. apt-get update && apt-get install -y apt-transport-https
2. curl https://mirrors.aliyun.com/kubernetes/apt/doc/apt-key.gpg | apt-key add - 
3. 将deb https://mirrors.aliyun.com/kubernetes/apt/ kubernetes-xenial main 加入到下面文件中，没有就创建
/etc/apt/sources.list.d/kubernetes.list
4.apt-get update
5.apt-get install -y kubelet kubeadm kubectl

centos
cat <<EOF > /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64
enabled=1
gpgcheck=1
repo_gpgcheck=1
gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg
EOF
# 安装
yum install -y kubelet kubeadm kubectl

三.配置Master节点
编写kubeadm.yaml
apiVersion: kubeadm.k8s.io/v1alpha1
kind: MasterConfiguration
controllerManagerExtraArgs:
  horizontal-pod-autoscaler-use-rest-clients: "true"
  horizontal-pod-autoscaler-sync-period: "10s"
  node-monitor-grace-period: "10s"
apiServerExtraArgs:
  runtime-config: "api/all=true"
kubernetesVersion: "stable-1.11"
其中
horizontal-pod-autoscaler-use-rest-clients: "true"表示kube-controller-manager可以使用自定义资源进行水平扩展。
使用 kubeadm init --config kubeadm.yaml 对master节点进行部署 
遇到问题：
your configuration file uses an old API spec: "kubeadm.k8s.io/v1alpha1". Please use kubeadm v1.11 instead and run 'kubeadm config migrate --old-config old.yaml --new-config new.yaml', which will write the new, similar spec using a newer API version.
原因是版本过高。kubeadm 可以查看版本
解决方法：1）重新安装低版本就好了
apt remove kubelet kubectl kubeadm
apt install kubelet=1.11.3-00
apt install kubectl=1.11.3-00
apt install kubeadm=1.11.3-00
2）配置文件调整为高版本的：
apiVersion: kubeadm.k8s.io/v1beta2
kind: ClusterConfiguration
controllerManager:
    extraArgs:
        horizontal-pod-autoscaler-use-rest-clients: "true"
        horizontal-pod-autoscaler-sync-period: "10s"
        node-monitor-grace-period: "10s"
apiServer:
    extraArgs:
        runtime-config: "api/all=true"
kubernetesVersion: "stable-1.15"

又被墙了...

error execution phase preflight: [preflight] Some fatal errors occurred:
	[ERROR ImagePull]: failed to pull image k8s.gcr.io/kube-apiserver:v1.15.3: output: Error response from daemon: Get https://k8s.gcr.io/v2/: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)
, error: exit status 1
	[ERROR ImagePull]: failed to pull image k8s.gcr.io/kube-controller-manager:v1.15.3: output: Error response from daemon: Get https://k8s.gcr.io/v2/: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)
, error: exit status 1
	[ERROR ImagePull]: failed to pull image k8s.gcr.io/kube-scheduler:v1.15.3: output: Error response from daemon: Get https://k8s.gcr.io/v2/: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)
, error: exit status 1
	[ERROR ImagePull]: failed to pull image k8s.gcr.io/kube-proxy:v1.15.3: output: Error response from daemon: Get https://k8s.gcr.io/v2/: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)
, error: exit status 1
	[ERROR ImagePull]: failed to pull image k8s.gcr.io/pause:3.1: output: Error response from daemon: Get https://k8s.gcr.io/v2/: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)
, error: exit status 1
	[ERROR ImagePull]: failed to pull image k8s.gcr.io/etcd:3.3.10: output: Error response from daemon: Get https://k8s.gcr.io/v2/: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)
, error: exit status 1
	[ERROR ImagePull]: failed to pull image k8s.gcr.io/coredns:1.3.1: output: Error response from daemon: Get https://k8s.gcr.io/v2/: net/http: request canceled 

解决办法：
docker search kube-apiserver 看到有哪些可以下载的
docker pull googlecontainer/kube-apiserver:v1.15.3
docker pull googlecontainer/kube-controller-manager:v1.15.3
docker pull mirrorgooglecontainers/kube-scheduler:v1.15.3
docker pull mirrorgooglecontainers/kube-proxy:v1.15.3
docker pull mirrorgooglecontainers/pause:3.1
docker pull mirrorgooglecontainers/etcd:3.3.10
docker pull coredns/coredns:1.3.1 

改标签：
docker tag googlecontainer/kube-apiserver:v1.15.3 k8s.gcr.io/kube-apiserver:v1.15.3
docker tag googlecontainer/kube-controller-manager:v1.15.3 k8s.gcr.io/kube-controller-manager:v1.15.3
docker tag mirrorgooglecontainers/kube-scheduler:v1.15.3 k8s.gcr.io/kube-scheduler:v1.15.3
docker tag mirrorgooglecontainers/kube-proxy:v1.15.3 k8s.gcr.io/kube-proxy:v1.15.3
docker tag mirrorgooglecontainers/pause:3.1 k8s.gcr.io/pause:3.1
docker tag mirrorgooglecontainers/etcd:3.3.10 k8s.gcr.io/etcd:3.3.10
docker tag coredns/coredns:1.3.1 k8s.gcr.io/coredns:1.3.1

docker login报错解决方案：
sudo apt install gnupg2 pass


再执行kubeadm init --config kubeadm.yaml，就显示安装成功了，按照提示操作：

Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config
执行这些命令是因为k8s集群默认需要加密方式进行访问，这里将刚刚部署生成的k8s集群的安全配置文件，保存在当前用户的.kube目录下。kubectl默认会使用这个目录下的授权信息访问k8s集群

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 192.168.3.27:6443 --token g4zy76.324xbav21n9jrukt \
    --discovery-token-ca-cert-hash sha256:67a8c652fc68450d9b7f413326f63c4e1213d57dfab44dff5e569b49cb031d49 
工作节点就可以通过这个命令加入到k8s集群

四.查看节点信息
kubectl get nodes
NAME                      STATUS     ROLES    AGE   VERSION
luoji-huawei-matebook-d   NotReady   master   19h   v1.15.3
发现状态为NotReady

kubectl describe node  luoji-huawei-matebook-d
可以查看节点信息，发现失败原因如下，是没有配置网络插件：
runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized
解决方法：
kubectl apply -f https://git.io/weave-kube-1.6
执行后再次查看pod状态和node信息
kubectl get pods -n kube-system
kubectl get nodes
NAME                      STATUS   ROLES    AGE   VERSION
luoji-huawei-matebook-d   Ready    master   19h   v1.15.3


五.部署worker node
只需安装docker和kubeadm后
执行加入master即可
kubeadm join 192.168.3.27:6443 --token g4zy76.324xbav21n9jrukt \
    --discovery-token-ca-cert-hash sha256:67a8c652fc68450d9b7f413326f63c4e1213d57dfab44dff5e569b49cb031d49 
默认情况下，master node是不允许创建用户pod的，只能在worker node上创建。如果要在master node创建就要使用到taint/toleration机制，但是一般不建议这样做。

六.部署k8s可视化插件dashboard
kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/master/aio/deploy/recommended/kubernetes-dashboard.yaml

七.部署容器存储插件Rook
为什么需要存储插件？
很多时候，我们会需要使用Volume（数据卷）把宿主机上的文件目录挂载到容器的Mount Namespace之中，从而达到共享的目的。
如果你在一台机器上启动一个机器，就无法看到其它机器的容器在数据卷里写入的内容。这也是容器的无状态特性。
有了持久化存储之后，存储插件会在容器里挂在一个远程数据卷，使得容器里创建的文件保存在远程服务器（可以是多个节点的分布式集群）上，这样无论那一条机器启动的容器，都可以通过请求去访问远程服务器保存的内容。这样就解耦了。
kubectl apply -f https://raw.githubusercontent.com/rook/rook/master/cluster/examples/kubernetes/ceph/operator.yaml
kubectl apply -f https://raw.githubusercontent.com/rook/rook/master/cluster/examples/kubernetes/ceph/cluster.yaml






