0. 网络栈
一个 Linux 容器能看见的“网络栈”，实际上是被隔离在它自己的 Network Namespace 当中的。
而所谓“网络栈”，就包括了：
网卡（Network Interface）、回环设备（Loopback Device）、路由表（Routing Table）和 iptables 规则。
对于一个进程来说，这些要素，其实就构成了它发起和响应网络请求的基本环境。

需要指出的是，作为一个容器，它可以声明直接使用宿主机的网络栈（–net=host），即：不开启 Network Namespace
$ docker run –d –net=host --name nginx-host nginx
在这种情况下，这个容器启动后，直接监听的就是宿主机的 80 端口。
像这样直接使用宿主机网络栈的方式，虽然可以为容器提供良好的网络性能，但也会不可避免地引入共享网络资源的问题，
比如端口冲突。所以，在大多数情况下，我们都希望容器进程能使用自己 Network Namespace 里的网络栈，
即：拥有属于自己的 IP 地址和端口。

1. 同一台宿主机上两个docker容器要通信，实际是使用了宿主机上的docker0网桥 + veth-pair技术。
veth-pair其两端可以进行通信（即使在不同的namespace之中，实际是使用veth-pair连通了容器和docker0网桥），
docker0网桥则充当了类似路由器的作用，把数据包转发到对应的容器里。
需要注意的是，docker里叫docker0网桥，但是在k8s里，是cni网桥（默认名称为cni0），并且网段可配。
（你可以把每一个容器看做一台主机，它们都有一套独立的“网络栈”。）
进入容器，使用ifconfig命令和route命令可以看到，
这个容器里有一张叫作 eth0 的网卡，它正是一个 Veth Pair 设备在容器里的这一端。

所以，当你遇到容器连不通“外网”的时候，你都应该先试试 docker0 网桥能不能 ping 通，
然后查看一下跟 docker0 和 Veth Pair 设备相关的 iptables 规则是不是有异常，往往就能够找到问题的答案了。

2. 同1，容器访问要访问另一台主机的时候，也是通过docker0/cni网桥，找到宿主机的网卡，再到另一台主机。

3. 一台宿主机1的容器的A访问另一台宿主机2的容器B，则需要一个“OverLay”网络，它是将容器连通的一个虚拟网络。

构建这种容器网络的核心在于：
我们需要在已有的宿主机网络上，再通过软件构建一个覆盖在已有宿主机网络之上的、可以把所有容器连通在一起的虚拟网络。
所以，这种技术就被称为：Overlay Network（覆盖网络）。

1）.Flannel UDP模式
flannel0是一个TUN设备，即工作在L3的虚拟网络设备，它的作用就是在操作系统内核和用户应用之间传递IP包。
当达到docker0/cni网桥时，发现目的地址时容器B的IP,于是转发给flannel0；
flannel0 在收到 container-1 发给 container-2 的 IP 包之后，就会把这个 IP 包直接封装在一个 UDP 包里，
然后发送给 Node 2。
这个 UDP 包的源地址，就是 flanneld 所在的 Node 1 的地址，而目的地址，则是 container-2 所在的宿主机 Node 2 的地址。
flannel0知道容器B在宿主机2，于是通过路由转发给宿主机2；宿主机2拿到包后，路由到它的flannel0，再转发给对应的容器B。
为什么flannel设备知道要发到宿主机2呢？因为宿主机X内的所有容器的IP一定是宿主机网段的子网。
每台宿主机上的 flanneld，都监听着一个 8285 端口，所以 flanneld 只要把 UDP 包发往 Node 2 的 8285 端口即可.
Node2 8225就会拆解UDP包，得到container-1发出的IP包，进而通过IP包找到目标容器container-2。

这个方案容易理解，但是也有缺点，就是一次通信需要3次用户态<->内核态的切换：
a.用户态容器进程发出IP包经过docker0网桥进入内核态
b.IP包经过路由表进入tun设备，从内核态进入到用户态的flannel进程
c.flannel进程进行重新封包后进入内核态，由Node1网卡发出到Node2
用户态和内核态交互势必会影响性能。

在由 Flannel 管理的容器网络里，一台宿主机上的所有容器，都属于该宿主机被分配的一个“子网”。
所以，flannel0知道容器B在宿主机2。而这些子网与宿主机的对应关系，正是保存在 Etcd 当中：
$ etcdctl ls /coreos.com/network/subnets
/coreos.com/network/subnets/100.96.1.0-24
/coreos.com/network/subnets/100.96.2.0-24
/coreos.com/network/subnets/100.96.3.0-24

$ etcdctl get /coreos.com/network/subnets/100.96.2.0-24
{"PublicIP":"10.168.0.3"}

总结一下，Flannel UDP 模式提供的其实是一个三层的 Overlay 网络，即：它首先对发出端的 IP 包进行 UDP 封装，
然后在接收端进行解封装拿到原始的 IP 包，进而把这个 IP 包转发给目标容器。
这就好比，Flannel 在不同宿主机上的两个容器之间打通了一条“隧道”，使得这两个容器可以直接使用 IP 地址进行通信，
而无需关心容器和宿主机的分布情况。

2）. Flannel VXLAN模式 
VXLAN，即 Virtual Extensible LAN（虚拟可扩展局域网），是 Linux 内核本身就支持的一种网络虚似化技术。
所以说，VXLAN 可以完全在内核态实现上述封装和解封装的工作，从而通过与前面相似的“隧道”机制，
构建出覆盖网络（Overlay Network）。

在现有的三层网络之上，覆盖一层虚拟的，由内核VXLAN模块负责的二层网络，使得连接在 VXLAN二层网络上的主机（虚拟机或容器）之间，
可以像局域网里一样通信。使用VTEP设备替换UDP模式中的flanne进程，
不过它进行封装和解封装的对象是二层数据帧，不涉及用户态和内核态的切换，因此提升了性能。

而为了能够在二层网络上打通“隧道”，VXLAN 会在宿主机上设置一个特殊的网络设备作为“隧道”的两端。
这个设备就叫作 VTEP，即：VXLAN Tunnel End Point（虚拟隧道端点）。

而 VTEP 设备的作用，其实跟前面的 flanneld 进程非常相似。只不过，它进行封装和解封装的对象，是二层数据帧（Ethernet frame）；
而且这个工作的执行流程，全部是在内核里完成的（因为 VXLAN 本身就是 Linux 内核中的一个模块）。

VXLAN 的 VTEP。如何建立呢？可以通过 netlink 通知内核建立一个 VTEP 的网卡 flannel.1。
netlink 是一种用户态和内核态通信的机制。
当网络包从物理机 A 上的容器 A 发送给物理机 B 上的容器 B，在容器 A 里面通过默认路由到达物理机 A 上的 docker0 网卡，
然后根据路由规则，在物理机 A 上，将包转发给 flannel.1。这个时候 flannel.1 就是一个 VXLAN 的 VTEP 了，
它将网络包进行封装。
内部的 MAC 地址这样写：源为物理机 A 的 flannel.1 的 MAC 地址，
目标为物理机 B 的 flannel.1 的 MAC 地址，在外面加上 VXLAN 的头。
外层的 IP 地址这样写：源为物理机 A 的 IP 地址，目标为物理机 B 的 IP 地址，外面加上物理机的 MAC 地址。
通过 VXLAN 将包转发到B，从物理机 B 的 flannel.1 上解包，变成内部的网络包，
通过物理机 B 上的路由转发到 docker0，然后转发到容器 B 里面。通信成功。

我们在进行系统级编程的时候，有一个非常重要的优化原则，就是要减少用户态到内核态的切换次数，并且把核心的处理逻辑都放在内核态进行。
这也是为什么，Flannel 后来支持的VXLAN 模式，逐渐成为了主流的容器网络方案的原因。

3）. Flannel host-gw模式
原理其实就是将每个Flannel子网的下一跳，设置成了该子网对应的宿主机的IP。
相当于主机会充当这两条容器同路径里的网关的角色，这也是为什么叫host-gw。

一旦配置了下一跳地址，那么当 IP 包从网络层进入链路层封装成帧的时候，
Node1的 eth0 设备就会使用下一跳IP 地址对应的 MAC 地址(arp获取)，作为该数据帧的目的 MAC 地址。
显然，这个 MAC 地址，正是 Node 2 的 MAC 地址。

Flannel 子网和主机的信息，都是保存在 Etcd 当中的。
flanneld 只需要 WACTH 这些数据的变化，然后实时更新路由表即可。

在host-gw模式下，容器通信的过程就免除了额外的封包和解包带来的性能损耗。
根据实际的测试，host-gw 的性能损失大约在 10% 左右，
而其他所有基于 VXLAN“隧道”机制的网络方案，性能损失都在 20%~30% 左右。

但是flannel host-gw模式必须要求集群主机之间是二层连通的。
host-gw 模式能够正常工作的核心，就在于 IP 包在封装成帧发送出去的时候，
会使用路由表里的“下一跳”来设置目的 MAC 地址。这样，它就会经过二层网络到达目的宿主机。


4.k8s的CNI网络插件
Kubernetes 创建一个 Pod 的第一步，就是创建并启动一个 Infra 容器，用来“hold”住这个 Pod 的 Network Namespace。
k8s在启动Infra容器之后，就可以直接调用CNI网络插件，为这个Infra容器的Network Namespace创建符合预期的网络栈（包括网卡，回环设备，路由表，iptables等）
安装时的kubernates-cni包就是安装了cni网络插件所需的基础执行文件，使其具备创建网络栈的能：

1）Main插件：创建具体网络设备的二进制文件，如网桥设备、Veth Pair、ipvlan、macvlan等
2）IPAM: 负责分配IP地址的二进制文件，比如DHCP
3）CNI社区维护的内置CNI插件：比如上面提到的flannel就是专门为flannel网络提供的CNI插件
由于 Flannel 项目对应的 CNI 插件已经被内置了，所以它无需再单独安装。
而对于 Weave、Calico 等其他项目来说，我们就必须在安装插件的时候，
把对应的 CNI 插件的可执行文件放在 /opt/cni/bin/ 目录下。

需要注意，Kubernetes 目前不支持多个 CNI 插件混用。
如果你在 CNI 配置目录（/etc/cni/net.d）里放置了多个 CNI 配置文件的话，dockershim 只会加载按字母顺序排序的第一个插件。
但另一方面，CNI 允许你在一个 CNI 配置文件里，通过 plugins 字段，定义多个插件进行协作。

5. Calico跨主机通信方案原理
在容器生态中，要说到像 Flannel host-gw 这样的三层网络方案，我们就不得不提到这个领域里的“龙头老大”Calico 项目了。
这个三层网络方案得以正常工作的核心，依然是为每个容器的 IP 地址，找到它所对应的、“下一跳”的网关。
类似：
<目的容器IP地址段> via <网关的IP地址> dev eth0

不同于 Flannel 通过 Etcd 和宿主机上的 flanneld 来维护路由信息的做法，
Calico 项目使用了一个“重型武器”来自动地在整个集群中分发路由信息,即BGP。

BGP 的全称是 Border Gateway Protocol，即：边界网关协议。
它是一个 Linux 内核原生就支持的、专门用在大规模数据中心里维护不同的“自治系统”之间路由信息的、无中心的路由协议。

假设有两个自治系统AS1和AS2，正常情况下自治系统之间不会有任何来往。
AS1和AS2想要通信，就需要一个或多个路由器将它们连起来，这些路由器就是边界网关。
这些路由器与一般路由器的不同在于:它的路由表里拥有其他自治系统里主机路由信息。
你可以认为，每个边界网关上都会运行一个小程序，对收到的数据进行分析，然后将需要的信息添加到自己的路由表里面。
所以，BGP，就是在大规模网络中实现节点路由信息共享的一种协议。

Calico项目不会在宿主机创建任何网桥，由BGP来维护路由信息。
这就是 Calico 网络的大概思路，即不走 Overlay 网络，不引入另外的网络性能损耗，
而是将转发全部用三层网络的路由转发来实现。

BGP 这种原生就是为大规模网络环境而实现的协议，其可靠性和可扩展性，远非 Flannel 自己的方案可比。


了解了 BGP 之后，Calico 项目的架构就非常容易理解了。它由三个部分组成：
Calico 的 CNI 插件。这是 Calico 与 Kubernetes 对接的部分。
Felix。它是一个 DaemonSet，负责在宿主机上插入路由规则（即：写入 Linux 内核的 FIB 转发信息库），以及维护 Calico 所需的网络设备等工作。
BIRD。它就是 BGP 的客户端，专门负责在集群里分发路由规则信息。


除了对路由信息的维护方式之外，Calico 项目与 Flannel 的 host-gw 模式的另一个不同之处，
就是它不会在宿主机上创建任何网桥设备。

Calico 的 CNI 插件会为每个容器设置一个 Veth Pair 设备，然后把其中的一端放置在宿主机上（它的名字以 cali 为前缀）
Calico 的 CNI 插件还需要在宿主机上为每个容器的 Veth Pair 设备配置一条路由规则，用于接收传入的 IP 包：
10.233.2.3 dev cali5863f3 scope link
即发往目的容器 10.233.2.3 的 IP 包，应该进入 cali5863f3 设备。
然后，宿主机网络栈就会根据路由规则的下一跳 IP 地址，把它们转发给正确的网关。
接下来的流程就跟 Flannel host-gw 模式完全一致了。