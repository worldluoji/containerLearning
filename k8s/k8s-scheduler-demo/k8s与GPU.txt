1. 背景
从 2016 年开始，Kubernetes 社区就不断收到来自不同渠道的大量诉求，
希望能够在 Kubernetes 集群上运行 TensorFlow 等机器学习框架所创建的训练（Training）和服务（Serving）任务。
而这些诉求中，除了前面我为你讲解过的 Job、Operator 等离线作业管理需要用到的编排概念之外，
还有一个亟待实现的功能，就是对 GPU 等硬件加速设备管理的支持。

2. GPU管理实现
对于云的用户来说，在 GPU 的支持上，他们最基本的诉求其实非常简单：我只要在 Pod 的 YAML 里面，
声明某容器需要的 GPU 个数，那么 Kubernetes 为我创建的容器里就应该出现对应的 GPU 设备，
以及它对应的驱动目录。

以 NVIDIA 的 GPU 设备为例，上面的需求就意味着当用户的容器被创建之后，这个容器里必须出现如下两部分设备和目录：
GPU 设备，比如 /dev/nvidia0；
GPU 驱动目录，比如 /usr/local/nvidia/*。

Kubernetes 在 Pod 的 API 对象里，并没有为 GPU 专门设置一个资源类型字段，
而是使用了一种叫作 Extended Resource（ER）的特殊字段来负责传递 GPU 的信息：

例如：
apiVersion: v1
kind: Pod
metadata:
  name: cuda-vector-add
spec:
  restartPolicy: OnFailure
  containers:
    - name: cuda-vector-add
      image: "k8s.gcr.io/cuda-vector-add:v0.1"
      resources:
        limits:
          nvidia.com/gpu: 1

在上述 Pod 的 limits 字段里，这个资源的名称是nvidia.com/gpu，它的值是 1。
也就是说，这个 Pod 声明了自己要使用一个 NVIDIA 类型的 GPU。

而在 kube-scheduler 里面，它其实并不关心这个字段的具体含义，只会在计算的时候，
一律将调度器里保存的该类型资源的可用量，直接减去 Pod 声明的数值即可。
所以说，Extended Resource，其实是 Kubernetes 为用户设置的一种对自定义资源的支持。

为了能够让调度器知道这个自定义类型的资源在每台宿主机上的可用量，宿主机节点本身，就必须能够向 API Server 汇报该类型资源的可用数量。
在 Kubernetes 里，各种类型的资源可用量，其实是 Node 对象 Status 字段的内容，比如下面这个例子：
apiVersion: v1
kind: Node
metadata:
  name: node-1
...
Status:
  Capacity:
   cpu:  2
   memory:  2049008Ki


为了能够在上述 Status 字段里添加自定义资源的数据，你就必须使用 PATCH API 来对该 Node 对象进行更新，加上你的自定义资源的数量。
这个 PATCH 操作，可以简单地使用 curl 命令来发起：

# 启动 Kubernetes 的客户端 proxy，这样你就可以直接使用 curl 来跟 Kubernetes  的API Server 进行交互了
$ kubectl proxy

# 执行 PACTH 操作
$ curl --header "Content-Type: application/json-patch+json" \
--request PATCH \
--data '[{"op": "add", "path": "/status/capacity/nvidia.com/gpu", "value": "1"}]' \
http://localhost:8001/api/v1/nodes/<your-node-name>/status

PATCH 操作完成后，你就可以看到 Node 的 Status 变成了如下所示的内容：

apiVersion: v1
kind: Node
...
Status:
  Capacity:
   cpu:  2
   memory:  2049008Ki
   nvidia.com/gpu: 1

当然，在 Kubernetes 的 GPU 支持方案里，你并不需要真正去做上述关于 Extended Resource 的这些操作。
在 Kubernetes 中，对所有硬件加速设备进行管理的功能，都是由一种叫作 Device Plugin 的插件来负责的。
这其中，当然也就包括了对该硬件的 Extended Resource 进行汇报的逻辑。

首先，对于每一种硬件设备，都需要有它所对应的 Device Plugin 进行管理，这些 Device Plugin，
都通过 gRPC 的方式，同 kubelet 连接起来。
以 NVIDIA GPU 为例，它对应的插件叫作NVIDIA GPU device plugin。
这个 Device Plugin 会通过一个叫作 ListAndWatch 的 API，定期向 kubelet 汇报该 Node 上 GPU 的列表。
比如，在我们的例子里，一共有三个 GPU（GPU0、GPU1 和 GPU2）。
这样，kubelet 在拿到这个列表之后，就可以直接在它向 APIServer 发送的心跳里，
以 Extended Resource 的方式，加上这些 GPU 的数量，比如nvidia.com/gpu=3。
所以说，用户在这里是不需要关心 GPU 信息向上的汇报流程的。

需要注意的是，ListAndWatch 向上汇报的信息，只有本机上 GPU 的 ID 列表，
而不会有任何关于 GPU 设备本身的信息。而且 kubelet 在向 API Server 汇报的时候，
只会汇报该 GPU 对应的 Extended Resource 的数量。
当然，kubelet 本身，会将这个 GPU 的 ID 列表保存在自己的内存里，并通过 ListAndWatch API 定时更新。


3. 问题
不过，正如同 TensorFlow 之于 Google 的战略意义一样，GPU 支持对于 Kubernetes 项目来说，其实也有着超过技术本身的考虑。
所以，尽管在硬件加速器这个领域里，Kubernetes 上游有着不少来自 NVIDIA 和 Intel 等芯片厂商的工程师，
但这个特性本身，却从一开始就是以 Google Cloud 的需求为主导来推进的。

这里最大的问题在于，GPU 等硬件设备的调度工作，实际上是由 kubelet 完成的。
即，kubelet 会负责从它所持有的硬件设备列表中，为容器挑选一个硬件设备，
然后调用 Device Plugin 的 Allocate API 来完成这个分配操作。

可以看到，在整条链路中，调度器扮演的角色，仅仅是为 Pod 寻找到可用的、支持这种硬件设备的节点而已。

这就使得，Kubernetes里对硬件设备的管理，只能处理“设备个数”这唯一一种情况。
一旦你的设备是异构的、不能简单地用“数目”去描述具体使用需求的时候，
比如，“我的 Pod 想要运行在计算能力最强的那个 GPU 上”，Device Plugin 就完全不能处理了。

此外，上述 Device Plugin 的设计，也使得 Kubernetes 里，缺乏一种能够对 Device 进行描述的 API 对象。
这就使得如果你的硬件设备本身的属性比较复杂，并且 Pod 也关心这些硬件的属性的话，
那么 Device Plugin 也是完全没有办法支持的。